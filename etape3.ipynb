{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c916df99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x240d151d790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# For BERT embeddings\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb81f528",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_sentence(tokens):\n",
    "    # tokens doit être une liste de strings\n",
    "    if isinstance(tokens, list):\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        # cas de tokens mal formé : on renvoie chaîne vide\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9b1a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r\"C:\\Users\\hp\\Desktop\\pfemaster\\dataset\\dataset_after_preprocessing.csv\", sep=',', encoding='utf-8')\n",
    "df['tokens_list'] = df['tokens_no_stopwords'].apply(ast.literal_eval)\n",
    "\n",
    "df['text_joined'] = df['tokens_list'].apply(rebuild_sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbd34d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "      <th>Dialect</th>\n",
       "      <th>tokens_list</th>\n",
       "      <th>text_joined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['اليكم', 'جديد', 'واخيرا', 'درت', 'لهاد', 'ال...</td>\n",
       "      <td>1</td>\n",
       "      <td>[اليكم, جديد, واخيرا, درت, لهاد, الصفحه]</td>\n",
       "      <td>اليكم جديد واخيرا درت لهاد الصفحه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['الله', 'يخلف', 'سيدي', 'محمد']</td>\n",
       "      <td>1</td>\n",
       "      <td>[الله, يخلف, سيدي, محمد]</td>\n",
       "      <td>الله يخلف سيدي محمد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['هجومك', 'اخي', 'علي', 'حسن', 'طارق', 'تبرهيش...</td>\n",
       "      <td>1</td>\n",
       "      <td>[هجومك, اخي, علي, حسن, طارق, تبرهيش, قله, عقل,...</td>\n",
       "      <td>هجومك اخي علي حسن طارق تبرهيش قله عقل ودليل ان...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['امين', 'عام', 'حزب', 'سياسي', 'كينشر', 'تدوي...</td>\n",
       "      <td>1</td>\n",
       "      <td>[امين, عام, حزب, سياسي, كينشر, تدوينه, وكيدير,...</td>\n",
       "      <td>امين عام حزب سياسي كينشر تدوينه وكيدير ليها</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['اجي', 'فين', 'غبرات', 'الحمي', 'القلاعيه', '...</td>\n",
       "      <td>1</td>\n",
       "      <td>[اجي, فين, غبرات, الحمي, القلاعيه, شفت, بقات, ...</td>\n",
       "      <td>اجي فين غبرات الحمي القلاعيه شفت بقات هضره عليها</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 tokens_no_stopwords  Dialect  \\\n",
       "0  ['اليكم', 'جديد', 'واخيرا', 'درت', 'لهاد', 'ال...        1   \n",
       "1                   ['الله', 'يخلف', 'سيدي', 'محمد']        1   \n",
       "2  ['هجومك', 'اخي', 'علي', 'حسن', 'طارق', 'تبرهيش...        1   \n",
       "3  ['امين', 'عام', 'حزب', 'سياسي', 'كينشر', 'تدوي...        1   \n",
       "4  ['اجي', 'فين', 'غبرات', 'الحمي', 'القلاعيه', '...        1   \n",
       "\n",
       "                                         tokens_list  \\\n",
       "0           [اليكم, جديد, واخيرا, درت, لهاد, الصفحه]   \n",
       "1                           [الله, يخلف, سيدي, محمد]   \n",
       "2  [هجومك, اخي, علي, حسن, طارق, تبرهيش, قله, عقل,...   \n",
       "3  [امين, عام, حزب, سياسي, كينشر, تدوينه, وكيدير,...   \n",
       "4  [اجي, فين, غبرات, الحمي, القلاعيه, شفت, بقات, ...   \n",
       "\n",
       "                                         text_joined  \n",
       "0                  اليكم جديد واخيرا درت لهاد الصفحه  \n",
       "1                                الله يخلف سيدي محمد  \n",
       "2  هجومك اخي علي حسن طارق تبرهيش قله عقل ودليل ان...  \n",
       "3        امين عام حزب سياسي كينشر تدوينه وكيدير ليها  \n",
       "4   اجي فين غبرات الحمي القلاعيه شفت بقات هضره عليها  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b47257f",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Créer les variables\n",
    "X = df['tokens_list'].values         # Pour BoW, TF-IDF → Token list\n",
    "X1 = df['text_joined'].values        # Pour Word2Vec, GloVe, BERT → Texte brut recomposé\n",
    "y = df['Dialect'].values             # Target identique pour les deux\n",
    "\n",
    "# Split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X1_train, X1_test, _, _ = train_test_split(\n",
    "    X1, y, test_size=0.2, random_state=42  # Même random_state pour cohérence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f2d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04bfead5",
   "metadata": {},
   "source": [
    " ===== TF-IDF VECTORIZATION ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8a7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_vectors(X_train, X_test, max_features=10000, ngram_range=(1, 2), save_path=None):\n",
    "  \n",
    "    print(f\"Creating TF-IDF vectors with {max_features} features, ngram_range={ngram_range}...\")\n",
    "    \n",
    "    # Initialize TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=max_features, \n",
    "                                       ngram_range=ngram_range,\n",
    "                                       min_df=5)\n",
    "    \n",
    "    # Fit and transform training data\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    # Transform testing data\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    print(f\"TF-IDF vectors created. Training shape: {X_train_tfidf.shape}, Testing shape: {X_test_tfidf.shape}\")\n",
    "    \n",
    "    # Save the vectorizer if path is provided\n",
    "    if save_path:\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(tfidf_vectorizer, f)\n",
    "        print(f\"TF-IDF vectorizer saved to {save_path}\")\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f325f3a",
   "metadata": {},
   "source": [
    "==== WORD2VEC VECTORIZATION ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b55671d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word2vec_model(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4, save_path=None):\n",
    "    \n",
    "    print(f\"Training Word2Vec model with vector_size={vector_size}, window={window}...\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    model = Word2Vec(sentences=tokenized_texts,\n",
    "                     vector_size=vector_size,\n",
    "                     window=window,\n",
    "                     min_count=min_count,\n",
    "                     workers=workers)\n",
    "    \n",
    "    print(f\"Word2Vec model trained. Vocabulary size: {len(model.wv.key_to_index)}\")\n",
    "    \n",
    "    # Save the model if path is provided\n",
    "    if save_path:\n",
    "        model.save(save_path)\n",
    "        print(f\"Word2Vec model saved to {save_path}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54636801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_vectors_w2v(tokenized_texts, word2vec_model, vector_size=100):\n",
    "\n",
    "    document_vectors = np.zeros((len(tokenized_texts), vector_size))\n",
    "\n",
    "    for i, tokens in enumerate(tqdm(tokenized_texts)):\n",
    "        vectors = [word2vec_model.wv[token] for token in tokens if token in word2vec_model.wv]\n",
    "        if vectors:\n",
    "            document_vectors[i] = np.mean(vectors, axis=0)\n",
    "\n",
    "    return document_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b3f87",
   "metadata": {},
   "source": [
    "===== GLOVE VECTORIZATION =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b208a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_path):\n",
    "   \n",
    "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "    glove_model = {}\n",
    "    vector_size = None\n",
    "    \n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            split_line = line.split()\n",
    "            word = split_line[0]\n",
    "            \n",
    "            # Get embedding dimension from first vector\n",
    "            if vector_size is None:\n",
    "                vector_size = len(split_line) - 1\n",
    "                \n",
    "            embedding = np.array([float(val) for val in split_line[1:]])\n",
    "            glove_model[word] = embedding\n",
    "    \n",
    "    print(f\"GloVe embeddings loaded. Vocabulary size: {len(glove_model)}, Vector size: {vector_size}\")\n",
    "    return glove_model, vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a379d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_document_vectors_glove(tokenized_texts, glove_model, vector_size):\n",
    "    \n",
    "    document_vectors = np.zeros((len(tokenized_texts), vector_size))\n",
    "\n",
    "    for i, tokens in enumerate(tqdm(tokenized_texts)):\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in glove_model:\n",
    "                vectors.append(glove_model[token])\n",
    "        \n",
    "        if vectors:\n",
    "            document_vectors[i] = np.mean(vectors, axis=0)\n",
    "\n",
    "    return document_vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18d8df",
   "metadata": {},
   "source": [
    "===== BERT EMBEDDINGS ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2222019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_bert_embeddings_from_tokens(tokenized_texts, model_name='UBC-NLP/MARBERTv2', batch_size=32, max_length=128):\n",
    "\n",
    "    print(f\"Generating BERT embeddings using {model_name} from tokenized texts...\")\n",
    "\n",
    "    # Charger le tokenizer et le modèle\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(tokenized_texts), batch_size)):\n",
    "        batch_tokens = tokenized_texts[i:i + batch_size]\n",
    "\n",
    "        # Convertir les listes de tokens en chaînes de texte\n",
    "        batch_texts = [\" \".join(tokens) for tokens in batch_tokens]\n",
    "\n",
    "        encoded_inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        encoded_inputs = {k: v.to(device) for k, v in encoded_inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_inputs)\n",
    "\n",
    "        # Utiliser l'embedding du token [CLS]\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"BERT embeddings created with shape: {embeddings.shape}\")\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54e9b03",
   "metadata": {},
   "source": [
    " ===== ARABIC-SPECIFIC BERT MODELS =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15904648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings_per_dialect(tokenized_texts, dialects, model_map, batch_size=32, max_length=128):\n",
    "\n",
    "    print(\"Génération des embeddings BERT selon le dialecte...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Préparer les modèles par dialecte\n",
    "    models = {}\n",
    "    tokenizers = {}\n",
    "\n",
    "    for dialect, model_name in model_map.items():\n",
    "        print(f\"Chargement du modèle pour {dialect} → {model_name}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "        tokenizers[dialect] = tokenizer\n",
    "        models[dialect] = model\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(tokenized_texts), batch_size)):\n",
    "        batch_tokens = tokenized_texts[i:i+batch_size]\n",
    "        batch_dialects = dialects[i:i+batch_size]\n",
    "\n",
    "        batch_embeddings = []\n",
    "\n",
    "        for tokens, dialect in zip(batch_tokens, batch_dialects):\n",
    "            text = \" \".join(tokens)\n",
    "            tokenizer = tokenizers[dialect]\n",
    "            model = models[dialect]\n",
    "\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(**encoded)\n",
    "                cls_embedding = output.last_hidden_state[:, 0, :].cpu().numpy().squeeze()\n",
    "                batch_embeddings.append(cls_embedding)\n",
    "\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    embeddings = np.array(embeddings)\n",
    "    print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f9216",
   "metadata": {},
   "source": [
    "===== UTILITY FUNCTIONS =====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b74a6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors(vectors, file_path):\n",
    "   \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(vectors, f)\n",
    "    print(f\"Vectors saved to {file_path}\")\n",
    "\n",
    "def load_vectors(file_path):\n",
    "   \n",
    "    with open(file_path, 'rb') as f:\n",
    "        vectors = pickle.load(f)\n",
    "    print(f\"Vectors loaded from {file_path} with shape: {vectors.shape}\")\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65a7e8",
   "metadata": {},
   "source": [
    "1. TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a33185ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF vectors with 10000 features, ngram_range=(1, 2)...\n",
      "TF-IDF vectors created. Training shape: (19761, 5652), Testing shape: (4941, 5652)\n",
      "TF-IDF vectorizer saved to tfidf_vectorizer.pkl\n",
      "Vectors saved to X_train_tfidf.pkl\n",
      "Vectors saved to X_test_tfidf.pkl\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf, X_test_tfidf, tfidf_vectorizer = create_tfidf_vectors(\n",
    "    X1_train, X1_test, max_features=10000, save_path=\"tfidf_vectorizer.pkl\"\n",
    ")\n",
    "save_vectors(X_train_tfidf, \"X_train_tfidf.pkl\")\n",
    "save_vectors(X_test_tfidf, \"X_test_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e7963",
   "metadata": {},
   "source": [
    "2. Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eef357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model with vector_size=100, window=5...\n",
      "Word2Vec model trained. Vocabulary size: 42421\n",
      "Word2Vec model saved to models/word2vec.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19761/19761 [00:00<00:00, 20381.87it/s]\n",
      "100%|██████████| 4941/4941 [00:00<00:00, 21826.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors saved to vectors/X_train_w2v.pkl\n",
      "Vectors saved to vectors/X_test_w2v.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_model = create_word2vec_model(\n",
    "    X_train, vector_size=100, save_path=\"models/word2vec.model\"\n",
    ")\n",
    "X_train_w2v = create_document_vectors_w2v(X_train, w2v_model, vector_size=100)\n",
    "X_test_w2v = create_document_vectors_w2v(X_test, w2v_model, vector_size=100)\n",
    "save_vectors(X_train_w2v, \"vectors/X_train_w2v.pkl\")\n",
    "save_vectors(X_test_w2v, \"vectors/X_test_w2v.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6a9883",
   "metadata": {},
   "source": [
    "3. GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6b64df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings from glove.6B.200d.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:24, 16025.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded. Vocabulary size: 400000, Vector size: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19761/19761 [00:00<00:00, 113500.68it/s]\n",
      "100%|██████████| 4941/4941 [00:00<00:00, 109350.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors saved to vectors/X_train_glove.pkl\n",
      "Vectors saved to vectors/X_test_glove.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "glove_model, vector_size = load_glove_model(\"glove.6B.200d.txt\")\n",
    "# Create document vectors\n",
    "\n",
    "X_train_glove = create_document_vectors_glove(X_train, glove_model, vector_size)\n",
    "X_test_glove = create_document_vectors_glove(X_test, glove_model, vector_size)\n",
    "\n",
    "save_vectors(X_train_glove, \"vectors/X_train_glove.pkl\")\n",
    "save_vectors(X_test_glove, \"vectors/X_test_glove.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df933dc6",
   "metadata": {},
   "source": [
    " 4. BERT Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d6d642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating BERT embeddings using UBC-NLP/MARBERTv2 from tokenized texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 618/618 [06:50<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings created with shape: (19761, 768)\n",
      "Generating BERT embeddings using UBC-NLP/MARBERTv2 from tokenized texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [01:34<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings created with shape: (4941, 768)\n",
      "Vectors saved to vectors/X_train_bert.pkl\n",
      "Vectors saved to vectors/X_test_bert.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard multilingual BERT\n",
    "\n",
    "X_train_bert2 = get_bert_embeddings_from_tokens(X_train, model_name=\"UBC-NLP/MARBERTv2\")\n",
    "X_test_bert2 = get_bert_embeddings_from_tokens(X_test, model_name=\"UBC-NLP/MARBERTv2\")\n",
    "\n",
    "save_vectors(X_train_bert2, \"vectors/X_train_bert.pkl\")\n",
    "save_vectors(X_test_bert2, \"vectors/X_test_bert.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d192d14",
   "metadata": {},
   "source": [
    "5. Arabic-specific BERT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3273d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des embeddings BERT selon le dialecte...\n",
      "Chargement du modèle pour 1 → SI2M-Lab/DarijaBERT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SI2M-Lab/DarijaBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle pour 0 → alger-ia/dziribert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle pour 2 → tunis-ai/TunBERT\n"
     ]
    }
   ],
   "source": [
    "# Exemple : mapping de chaque dialecte à un modèle BERT spécifique\n",
    "model_map = {\n",
    "    1: \"SI2M-Lab/DarijaBERT\",\n",
    "    0: \"alger-ia/dziribert\",\n",
    "    2: \"tunis-ai/TunBERT\"\n",
    "}\n",
    "\n",
    "\n",
    "# Génère les embeddings\n",
    "X_train_bert = get_bert_embeddings_per_dialect(X_train, y_train, model_map)\n",
    "X_test_bert = get_bert_embeddings_per_dialect(X_test, y_test, model_map)\n",
    "save_vectors(X_train_bert, \"vectors/X_train_bert.pkl\")\n",
    "save_vectors(X_test_bert, \"vectors/X_test_bert.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d415acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors saved to vectors/y_train.pkl\n",
      "Vectors saved to vectors/y_test.pkl\n"
     ]
    }
   ],
   "source": [
    "save_vectors(y_train, \"vectors/y_train.pkl\")\n",
    "save_vectors(y_test, \"vectors/y_test.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf9b6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff36bedd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
